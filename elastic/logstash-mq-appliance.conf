# ----------------------------------------------------------------------
# Sample Logstash 6.5.0 pipeline for IBM MQ Appliance syslog log targets
#
# Copyright 2018, 2020 IBM Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ----------------------------------------------------------------------

# --------------------------------------------------------------------
# Input: Listen for syslog information on both TCP and UDP
# --------------------------------------------------------------------

input {

  tcp {
    id   => "syslog_tcp"
    port => 5000
  }

  udp {
    id   => "syslog_udp"
    port => 5000
  }
}

# --------------------------------------------------------------------
# Filter: Parse the syslog messages sent from MQ Appliances so we have
#         separate fields for key information
#
#         Rather than have a single very complex grok we split it up
#         in to stages that parse sections of a syslog message in turn
# --------------------------------------------------------------------

filter {

  # ---------------------------------------------------------------
  # First parse the syslog message PRI field (priority information)
  #
  # The following fields are implicitly added:
  #   - syslog_facility
  #   - syslog_facility_code
  #   - syslog_severity
  #   - syslog_severity_code
  #
  # We also add fields to track when and where this log message was
  # received by logstash
  # ----------------------------------------------------------------

  syslog_pri {
    id        => "syslog_pri"
    add_field => [ "received-at",   "%{@timestamp}" ]
    add_field => [ "received-from", "%{host}" ]
  }

  # ----------------------------------------------------------------
  # Rename/remove the fields added by syslog_pri
  # ----------------------------------------------------------------

  mutate {
    id                  => "mutate_syslog_pri"
    rename              => {
      "syslog_facility" => "syslog-facility"
      "syslog_severity" => "syslog-severity"
    }
    remove_field        => [ "syslog_facility_code", "syslog_severity_code" ]
  }

  # ----------------------------------------------------------------------------------
  # Remove the PRI field and extract the timestamp in either RFC3164 or ISO8601 format
  # ----------------------------------------------------------------------------------

  grok {
    id             => "grok_timestamp"
    break_on_match => true
    match          => {
                        "message" => [
                                       "^<\d+>%{SYSLOGTIMESTAMP:timestamp} %{GREEDYDATA:message}$",
                                       "^<\d+>%{TIMESTAMP_ISO8601:timestamp} %{GREEDYDATA:message}$" 
                                     ]
                       }
    overwrite      => [ "message" ]
  }

  # -----------------------------------------------------------------
  # Parse the timestamp string and use it to set the @timestamp field
  # We keep the textual timestamp field in case it might be useful
  # -----------------------------------------------------------------

  date {
    id    => "date_timestamp"
    match => [ "timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss", "ISO8601" ]
  }

  # ----------------------------------------------------------------
  # Extract the hostname
  # ----------------------------------------------------------------

  grok {
    id             => "grok_hostname"
    match          => { "message" => "^%{SYSLOGHOST:hostname} %{GREEDYDATA:message}$" }
    overwrite      => [ "message" ]
  }

  # ----------------------------------------------------------------
  # Extract the message ID, category and loglevel
  # ----------------------------------------------------------------

  grok {
    id        => "grok_msgid_category_loglevel"
    match     => { "message" => "^\[%{DATA:msgid}\]\[%{DATA:category}\]\[%{DATA:loglevel}\] %{GREEDYDATA:message}$" }
    overwrite => [ "message" ]
  }

  # ------------------------------------------------------------------
  # Extract the object type and name (if present)
  #
  # Do not match if the object type is 'trans' because this is not an
  # object type, but a transaction identifier that is parsed next.
  # ------------------------------------------------------------------

  grok {
    id             => "grok_object"
    match          => { "message" => "^(?!trans)(?:%{DATA:object-type}\(%{DATA:object}\): )%{GREEDYDATA:message}$" }
    overwrite      => [ "message" ]
    tag_on_failure => []
  }

  # ------------------------------------------------------------------
  # Extract information about the client and transaction (if present)
  #
  # This rule matches two different patterns in the remaining message
  # text. The first pattern matches the presence of a transaction ID
  # followed by the transaction type, client IP and global transaction
  # identifier. The second pattern matches the presence of just a 
  # client IP - it is used if the first pattern does not match.
  # -----------------------------------------------------------------

  grok {
    id             => "grok_transaction"
    break_on_match => true 
    match          => {
                        "message" => [
                                       "^trans\(%{NUMBER:transaction:int}\)(?:\[(?![^\]]*\d)%{DATA:transaction-type}\])?(?:\[%{IP:client}\])?(?: gtid\(%{DATA:gtid}\))?: %{GREEDYDATA:message}$",
                                       "^(?:\[%{IP:client}\])?: %{GREEDYDATA:message}$"
                                     ]
                      }
    overwrite      => [ "message" ]
    tag_on_failure => []
  }

  # ---------------------------------------
  # Additional processing for MQ log events
  # ---------------------------------------

  if [category] == "qmgr" {

    # -------------------------------------------------------------------
    # From version 9.2 the MQ log events can contain extended information
    # at the end of the message text, including the insert values.
    # This extended information has the format:
    #   [ name(value), name(value), ... ]
    # If present, we parse the extended information in to separate fields
    # -------------------------------------------------------------------

    # ------------------------------------------------------------
    # First, extract any extended information to a separate field.
    # Some messages contain square brackets so we check the data
    # appears to have the expected format before capturing it.
    # ------------------------------------------------------------

    grok {
      id             => "grok_mq_extended"
      match          => { "message" => "^%{DATA:message}\s*\[(?=\w+\(.*\))%{DATA:mq-extended}\]$" }
      overwrite      => [ "message" ]
      tag_on_failure => []
    }

    # ------------------------------------------------------------------
    # Now extract the extended message insert fields from last to first.
    # We do this to protect against any values that contain parentheses,
    # which might otherwise cause incorrect parsing of the data.
    # ------------------------------------------------------------------

    # -------------------------------------
    # Extract comment insert 3 (if present)
    # -------------------------------------

    grok {
      id                  => "grok_mq_commentinsert3"
      match               => { "mq-extended" => "^%{DATA:mq-extended}(?:, )?CommentInsert3\(%{DATA:mq-commentinsert3}\)$" }
      overwrite           => [ "mq-extended" ]
      keep_empty_captures => true
      tag_on_failure      => []
    }

    # -------------------------------------
    # Extract comment insert 2 (if present)
    # -------------------------------------

    grok {
      id                  => "grok_mq_commentinsert2"
      match               => { "mq-extended" => "^%{DATA:mq-extended}(?:, )?CommentInsert2\(%{DATA:mq-commentinsert2}\)$" }
      overwrite           => [ "mq-extended" ]
      keep_empty_captures => true
      tag_on_failure      => []
    }

    # -------------------------------------
    # Extract comment insert 1 (if present)
    # -------------------------------------

    grok {
      id                  => "grok_mq_commentinsert1"
      match               => { "mq-extended" => "^%{DATA:mq-extended}(?:, )?CommentInsert1\(%{DATA:mq-commentinsert1}\)$" }
      overwrite           => [ "mq-extended" ]
      keep_empty_captures => true
      tag_on_failure      => []
    }

    # ----------------------------------------
    # Extract arithmetic insert 2 (if present)
    # ----------------------------------------

    grok {
      id                  => "grok_mq_arithinsert2"
      match               => { "mq-extended" => "^%{DATA:mq-extended}(?:, )?ArithInsert2\(%{NUMBER:mq-arithinsert2:int}\)$" }
      overwrite           => [ "mq-extended" ]
      keep_empty_captures => true
      tag_on_failure      => []
    }

    # ----------------------------------------
    # Extract arithmetic insert 1 (if present)
    # ----------------------------------------

    grok {
      id                  => "grok_mq_arithinsert1"
      match               => { "mq-extended" => "^%{DATA:mq-extended}(?:, )?ArithInsert1\(%{NUMBER:mq-arithinsert1:int}\)$" }
      overwrite           => [ "mq-extended" ]
      keep_empty_captures => true
      tag_on_failure      => []
    }

    # -----------------------------------------------------------------
    # Remove the mq-extended field if it doesn't contain any other data
    # -----------------------------------------------------------------

    grok {
      id             => "grok_prune_mq_extended"
      match          => { "mq-extended" => "^\s*$" }
      remove_field   => [ "mq-extended" ]
      tag_on_failure => []
    }
  }
}

# --------------------------------------------------------------------
# Output: Forward the syslog information to Elasticsearch and/or STDOUT
# --------------------------------------------------------------------

output {

  # -------------
  # Elasticsearch
  # -------------

  elasticsearch {
    id    => "elasticsearch"
    hosts => ["elasticsearch:9200"]
    index => "mqappliance-%{+YYYY.MM.dd}"
    codec => "plain"
  }

  # --------------------
  # STDOUT for debugging
  # --------------------

  # stdout { 
  #   id    => "stdout"
  #   codec => "rubydebug"
  # }
}
